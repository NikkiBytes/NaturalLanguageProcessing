{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e79c227e-ed88-4da7-9d77-dc23c48636d3",
   "metadata": {},
   "source": [
    "# Text Classification with SpaCy  \n",
    "\n",
    "A common task in NLP is **text classification**. This is classification in the conventional machine learning sense, and it is applied to text. Common examples include **spam detection**, **sentiment analysis**, and **tagging customer queries**.  \n",
    "    \n",
    "This tutorial is used to learn text classification with spaCy. The classifier will detect spam messages, a common functionality in most email clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ffce2c-b300-434f-8def-58e0e36ca8d6",
   "metadata": {},
   "source": [
    "## Bag of Words  \n",
    "In machine learning we have to convert text to numeric.  \n",
    "  \n",
    "The simplest common representation is a variation of **one-hot encoding**. In this, each document is represented as a vector of term frequencies for each term in the vocabulary. The vocabulary is built from all the tokens (terms) in the corpus (the collection of documents).  \n",
    "  \n",
    "As an example, the sentences: \"Tea is life. Tea is love.\" and \"Tea is healthy calming, and delicious.\" **as our corpus.** The **vocabulary** is then: `{\"tea\", \"is\", \"life\", \"love\", \"healthy\", \"calming\", \"and\", \"delicious\"}` (ignoring punctuation).   \n",
    "\n",
    "  \n",
    "For each document, count up how many times a term occurs, and place that count in the appropriate element of a vector. The first sentence has \"tea\" twice and the that is the first position in our vocabulary, so we put the number 2 in the first element of the vector. Our sentences as vectors then look like `v1=[22110000]` `v2=[11001111]`. This is called the **bag of words** representation. You can see that documents with similar terms will have similar vectors. Vocabularies frequently have tens of thousands of terms, so these vectors can be very large.  \n",
    "  \n",
    "Another common representation is **TF-IDF (Term Frequency-Inverse Document Frequency)**. TD-IDF is similar to bag of words except that each term is scaled by the term's frequency in the corpus. Using TF-IDF can potentially improve your models.  \n",
    "\n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab6c0e5-d246-4eff-a40b-6a44cd64186b",
   "metadata": {},
   "source": [
    "## Building a Bag of Words model  \n",
    "Once you have your documents in a bag of words representation, **those vectors can be used as input to any machine learning model.** spaCy handles the bag of words conversion and building a simple linear model for you with the `TextCategorizer` class.  \n",
    "  \n",
    "\n",
    "The `TextCategorizer` is a spaCy **pipe**. Pipes are <u> classes for processing and transforming tokens. </u> When you create a spaCy model with `nlp = spacy.load('en_core_web_sm')`, there are default pipes that perform **part of speech tagging**, **entity recognition**, and other transformations. When you run text through a model `doc = nlp(\"Some text here\")`, the output of the pipes are attached to the tokens in the `doc` object. The lemmas for `token.lemma_` come from one of these pipes.  \n",
    "  \n",
    "You can remove or add pipes to models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714d77ca-c1a1-42c8-a7ec-d48bc31785f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy  \n",
    "\n",
    "# Create an empty model  \n",
    "nlp = spacy.blank(\"en\")\n",
    "\n",
    "# Create the TextCategorizer with exclusive classes and \"bow\" architecture  \n",
    "textcat = nlp.create_pipe(\"textcat\", config={\"exclusive_classes\": True, \"architecture\" : \"bow\"})\n",
    "\n",
    "# Add the TextCategorizer to the empty model \n",
    "nlp.add_pipe(textcat)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2fcb19-768b-4afc-9d4b-8ca58a344500",
   "metadata": {},
   "source": [
    "Classes will be ham or spam, so we set `\"exclusive_classes\"` to `True`. We've also configured it with the bag of words (`\"bow\"`) architecture. spaCy provides a CNN architecture too.  \n",
    "\n",
    "Now we'll add labels to the model. The \"ham\" will be for the real messages, \"spam\" are for the spam messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb789aaf-6a25-4061-b2f0-8818aa7cf4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add labels to the text classifier\n",
    "textcat.add_label(\"ham\") \n",
    "textcat.add_label(\"spam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3456ae46-2430-44aa-a21c-4115663b6b2e",
   "metadata": {},
   "source": [
    "## Training a Text Categorizer Model  \n",
    "Next, convert the labels in the data to the form TextCategorizer requires.  \n",
    "For each document, you'll create a dictionary of boolean values for each class.  \n",
    "  \n",
    "For example, if a text is \"ham\", we need a dictionary `{'ham': True, 'spam': False}`.\n",
    "The model is looking for these labels inside another dictionary with the key 'cats'.  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c90c90-2776-43f3-b3f8-a574e6bef92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = spam['text'].values\n",
    "train_labels = [{'cats': {'ham': label == 'ham',\n",
    "                          'spam': label == 'spam'}} \n",
    "                for label in spam['label']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4778dc5-1221-4444-a769-f83e5beff5dc",
   "metadata": {},
   "source": [
    "Then we combine the texts and labels into a single list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c4cbe6-806c-44a1-95e4-f324eced1b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(zip(train_texts, train_labels))\n",
    "train_data[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7713830-fe4c-4515-a652-5084dcc4898d",
   "metadata": {},
   "source": [
    "Now we train the model. First, create an `optimizer` using `nlp.begin_training()`. spaCy uses this optimizer to update the model. In general it's more efficient to train models in small batches. spaCy provides the `minibatch` function that returns a generator yielding minibatches for training. Finally, the minibatches are split into texts and labels, then used with `nlp.update` to update the model's parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedab70a-7732-4582-8f74-bf25a8a1b5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Create the batch generator with batch size = 8\n",
    "batches = minibatch(train_data, size=8)\n",
    "# Iterate through minibatches\n",
    "for batch in batches:\n",
    "    # Each batch is a list of (text, label) but we need to\n",
    "    # send separate lists for texts and labels to update().\n",
    "    # This is a quick way to split a list of tuples into lists\n",
    "    texts, labels = zip(*batch)\n",
    "    nlp.update(texts, labels, sgd=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8a18fb3-40e2-4088-81d4-6f3674011f91",
   "metadata": {},
   "source": [
    "Now you are ready to train the model.  \n",
    "- First, create an `optimizer` using `nlp.begin_training()`. spaCy uses this optimizer to update the model. spaCy provides the `minibatch` function that returns a generator yielding minibatches for training.  \n",
    "- Finally, the minibatches are split into texts and labels, then used with `nlp.update` to update the model's parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b31ad6b-cc65-45ce-a4de-dd49499e5abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch\n",
    "\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# Create the batch generator with batch size = 8\n",
    "batches = minibatch(train_data, size=8)\n",
    "# Iterate through minibatches\n",
    "for batch in batches:\n",
    "    # Each batch is a list of (text, label) but we need to\n",
    "    # send separate lists for texts and labels to update().\n",
    "    # This is a quick way to split a list of tuples into lists\n",
    "    texts, labels = zip(*batch)\n",
    "    nlp.update(texts, labels, sgd=optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3c5c12-13e1-4486-9b2f-41ffdb5a9f65",
   "metadata": {},
   "source": [
    "This is just one training loop (or epoch) through the data. The model will typically need multiple epochs. Use another loop for more epochs, and optionally re-shuffle the training data at the beginning of each loop. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b386ea4a-972d-4195-b6b3-f6e753f550ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "spacy.util.fix_random_seed(1)\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "losses = {}\n",
    "for epoch in range(10):\n",
    "    random.shuffle(train_data)\n",
    "    # Create the batch generator with batch size = 8\n",
    "    batches = minibatch(train_data, size=8)\n",
    "    # Iterate through minibatches\n",
    "    for batch in batches:\n",
    "        # Each batch is a list of (text, label) but we need to\n",
    "        # send separate lists for texts and labels to update().\n",
    "        # This is a quick way to split a list of tuples into lists\n",
    "        texts, labels = zip(*batch)\n",
    "        nlp.update(texts, labels, sgd=optimizer, losses=losses)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544b160e-0574-4af0-a02d-c8920db9c19f",
   "metadata": {},
   "source": [
    "## Making Predicitons  \n",
    "Now that you have a trained model, you can make predictions with the `predict()` method. The input text needs to be tokenized with `nlp.tokenizer`. Then you pass the tokens to the predict method which returns scores. The scores are the probability the input text belongs to the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b771f46a-2b27-4a8b-87c5-314dd0559c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"Are you ready for the tea party????? It's gonna be wild\",\n",
    "         \"URGENT Reply to this message for GUARANTEED FREE TEA\" ]\n",
    "docs = [nlp.tokenizer(text) for text in texts]\n",
    "    \n",
    "# Use textcat to get the scores for each doc\n",
    "textcat = nlp.get_pipe('textcat')\n",
    "scores, _ = textcat.predict(docs)\n",
    "\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ce9bc-2742-48d2-8848-5b18be3b04f4",
   "metadata": {},
   "source": [
    "The scores are used to predict a single class or label by choosing the label with the highest probability. You get the index of the highest probability with `scores.argmax` , then use the index to get the label string from `textcat.labels`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1502ab52-5f98-4c66-8cb4-1db29b4c5100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the scores, find the label with the highest score/probability\n",
    "predicted_labels = scores.argmax(axis=1)\n",
    "print([textcat.labels[label] for label in predicted_labels])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afe497f-061b-4c19-b067-7b699644a188",
   "metadata": {},
   "source": [
    "Evaluating the model is straightforward once you have the predictions. To measure the accuracy, calculate how many correct predictions are made on some test data, divided by the total number of predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008c7818-d727-48c5-b887-9b7951b4e35e",
   "metadata": {},
   "source": [
    "## Reference  \n",
    "https://www.kaggle.com/matleonard/text-classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d36bd0f-f1c5-4ff6-8092-03aa85b1a4d6",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
