{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6984153-7297-4518-a6e5-98af028cf63d",
   "metadata": {},
   "source": [
    "# NLP with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723a1e67-5da3-4f65-be30-f51ffa6ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1feed3-83e9-41ff-9367-9e44f6136c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_obj=spacy.load('en_core_web_sm') # create a spaCy obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b117166-d36d-4328-9be8-4a25c4efc33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x260dddfa6d0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x260e0c1e860>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x260df097580>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x260df097520>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x260e0cb76c0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x260e0cb9cc0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_obj.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8a9c7f-1fe5-4959-ad1d-d65809d1756b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_obj.pipe_names # view the pipeline component names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed04d26-3dcb-4b45-abbf-3bcd27dd73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to disable pipeline components we use the disable_pipes()\n",
    "#spacy_obj.disable('attribute_ruler', 'tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49571cb0-d4b4-4270-8fc3-95a48d16dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a text into the spaCy object  \n",
    "text='''Shall I compare thee to a summer’s day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer’s lease hath all too short a date:\n",
    "Sometime too hot the eye of heaven shines,\n",
    "And often is his gold complexion dimm’d;\n",
    "And every fair from fair sometime declines,\n",
    "By chance or nature’s changing course untrimm’d;\n",
    "But thy eternal summer shall not fade\n",
    "Nor lose possession of that fair thou owest;\n",
    "Nor shall Death brag thou wander’st in his shade,\n",
    "When in eternal lines to time thou growest:\n",
    "So long as men can breathe or eyes can see,\n",
    "So long lives this and this gives life to thee.\n",
    "'''\n",
    "\n",
    "doc=spacy_obj(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba489520-ebf4-4b20-9c53-cc8af5af7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a summer’s day?\n",
      "Thou art more lovely and more temperate:\n",
      "Rough winds do shake the darling buds of May,\n",
      "And summer’s lease hath all too short a date:\n",
      "Sometime too hot the eye of heaven shines,\n",
      "And often is his gold complexion dimm’d;\n",
      "And every fair from fair sometime declines,\n",
      "By chance or nature’s changing course untrimm’d;\n",
      "But thy eternal summer shall not fade\n",
      "Nor lose possession of that fair thou owest;\n",
      "Nor shall Death brag thou wander’st in his shade,\n",
      "When in eternal lines to time thou growest:\n",
      "So long as men can breathe or eyes can see,\n",
      "So long lives this and this gives life to thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46145387-ddfc-43dd-9be9-cf2d0dab136a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30db96b-2ccf-4563-9174-aefbe19287b4",
   "metadata": {},
   "source": [
    "## 1. Tokenization  \n",
    "Split strings into tokens. Tokens can be a word or a character.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff108cf-2770-4167-ada7-fb90863c008d",
   "metadata": {},
   "source": [
    "**(A) Tokenization into sentences**  \n",
    "Use the `'sents'` attribute to tokenize the text doc into sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f836613-90cf-45a8-8869-f884b73125e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a summer’s day?\n",
      "\n",
      "\n",
      "Thou art more lovely and more temperate:\n",
      "\n",
      "Rough winds do shake the darling buds of May,\n",
      "\n",
      "And summer’s lease hath all too short a date:\n",
      "\n",
      "Sometime too hot the eye of heaven shines,\n",
      "And often is his gold complexion dimm’d;\n",
      "\n",
      "And every fair from fair sometime declines,\n",
      "\n",
      "By chance or nature’s changing course untrimm’d;\n",
      "\n",
      "But thy eternal summer shall not fade\n",
      "\n",
      "Nor lose possession of that fair thou owest;\n",
      "\n",
      "Nor shall Death brag thou wander’st in his shade,\n",
      "\n",
      "When in eternal lines to time thou growest:\n",
      "So long as men can breathe or eyes can see,\n",
      "\n",
      "So long lives this and this gives life to thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentences in doc.sents:\n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4566c-2c85-4a43-99e7-3d5f30e2d0d2",
   "metadata": {},
   "source": [
    "**(B) Tokenization into individual words**  \n",
    "This will divide the entire sentence into word by word, including the punctuation marks and escape sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5493d93-4694-40d6-82dd-e981a6dc593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall\n",
      "I\n",
      "compare\n",
      "thee\n",
      "to\n",
      "a\n",
      "summer\n",
      "’s\n",
      "day\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678b6674-ef5d-4cfe-b3af-448b85fcdf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall\n",
      "thee\n"
     ]
    }
   ],
   "source": [
    "# print individual tokens by using the slicing notation\n",
    "print(doc[0])\n",
    "print(doc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8038d-5253-4770-91b3-71f3e2bbf413",
   "metadata": {},
   "source": [
    "spaCy follows a specific rule for tokenization of text  \n",
    "1. Initially it starts preprocessing the raw text into tokens from left to right based on whitespaces.  \n",
    "2. Then it performs spliting tokens into sub-tokens by performing two different checks that are:  \n",
    "  - (a) Exception rule check: punctuation marks in between tokens are looked over and are left untokenised.  \n",
    "  - (b) Prefix-Suffix and Infix check: punctuation marks like commas, hyphens, quotation marks, periods, etc. are identified and made as a separate token.  \n",
    "  \n",
    "    \n",
    "This rule checking is applied iteratively on the tokens from left to right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a0f7383-93b0-42e7-8791-7072eeb54223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "love\n",
      "cats\n",
      "and\n",
      "coffee\n",
      ",\n",
      "\"\n",
      "I\n",
      "'ll\n",
      "get\n",
      "a\n",
      "big\n",
      "house\n",
      "for\n",
      "lots\n",
      "of\n",
      "cats\n",
      "in\n",
      "the\n",
      "U.S.A\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "test_text=spacy_obj(\"I love cats and coffee, \\\"I'll get a big house for lots of cats in the U.S.A \\\"\")\n",
    "\n",
    "for token in test_text:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0545095-160e-4c9f-9d39-bbdd4c2f02b2",
   "metadata": {},
   "source": [
    "- spaCy identifies each quotation mark, commas, question mark and other punctuation marks that are present in form of prefix, suffix, infix and separates them into an indvidual token. (In above example, I'll be was separated into 'I & 'll' two different tokens.)    \n",
    "- Punctuation marks that exists as part of a known abbereviation will not be separated. (In above example, U.S.A is kept as U.S.A.)  \n",
    "-Also, punctuation marks used as infixes will be exempted from tokenisation in cases of email address, website or some numerical figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8133342-3b9d-41a2-9ff2-027d702e82d8",
   "metadata": {},
   "source": [
    "**(C) Spanning/Slicing of words in the text**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d12575ae-6c61-4bb2-99b0-d7ce3d18ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a summer\n"
     ]
    }
   ],
   "source": [
    "print(doc[0:7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7ad76-c1c4-4833-b01e-8a08ec74df8c",
   "metadata": {},
   "source": [
    "To check whether a particular word is the starting of a sentence we can use the 'is_sent_start' attribute with the doc object. It returns a Boolean value, True in case the word is the starting of any sentence otherwise it returns False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7100847-4f20-4e45-8234-3f313494cad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(doc[0].is_sent_start)\n",
    "print(doc[1].is_sent_start)\n",
    "print(doc[10].is_sent_start)\n",
    "print(doc[18].is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52fab8-67ee-48b1-a9e5-98c33f683f16",
   "metadata": {},
   "source": [
    "**(D) Assignment of tokens is not allowed in spaCy.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07a000e2-f385-4dae-9a8d-e0c3e2fc66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc[0] = doc[1] # will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764fb7e-3046-4149-b3f0-0c3dd076923f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60199e41-350b-4cff-bb2e-6b1177cfc453",
   "metadata": {},
   "source": [
    "## 2. Lemmatization  \n",
    "Lemmatization is the process of grouping inflected words from a common root word. This groups them into a single term for analysis. It considers the language's full vocabulary so it can apply morphological analysis on words.  \n",
    "  \n",
    "**The point to be noted is that spaCy library does not have stemming feature, because it prefers lemmitization as it is considerd to be more informative than stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "270a5140-255e-4e04-aafa-98fe7737f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall -------> AUX -------> shall\n",
      "I -------> PRON -------> I\n",
      "compare -------> VERB -------> compare\n",
      "thee -------> PRON -------> thee\n",
      "to -------> ADP -------> to\n",
      "a -------> DET -------> a\n",
      "summer -------> NOUN -------> summer\n",
      "’s -------> PART -------> ’s\n",
      "day -------> NOUN -------> day\n",
      "? -------> PUNCT -------> ?\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "Thou -------> DET -------> thou\n",
      "art -------> NOUN -------> art\n",
      "more -------> ADV -------> more\n",
      "lovely -------> ADJ -------> lovely\n",
      "and -------> CCONJ -------> and\n",
      "more -------> ADV -------> more\n",
      "temperate -------> NOUN -------> temperate\n",
      ": -------> PUNCT -------> :\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "Rough -------> ADJ -------> rough\n",
      "winds -------> NOUN -------> wind\n",
      "do -------> AUX -------> do\n",
      "shake -------> VERB -------> shake\n",
      "the -------> DET -------> the\n",
      "darling -------> NOUN -------> darling\n",
      "buds -------> NOUN -------> bud\n",
      "of -------> ADP -------> of\n",
      "May -------> PROPN -------> May\n",
      ", -------> PUNCT -------> ,\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "And -------> CCONJ -------> and\n",
      "summer -------> NOUN -------> summer\n",
      "’s -------> PART -------> ’s\n",
      "lease -------> NOUN -------> lease\n",
      "hath -------> NOUN -------> hath\n",
      "all -------> ADV -------> all\n",
      "too -------> ADV -------> too\n",
      "short -------> ADJ -------> short\n",
      "a -------> DET -------> a\n",
      "date -------> NOUN -------> date\n",
      ": -------> PUNCT -------> :\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "Sometime -------> ADV -------> sometime\n",
      "too -------> ADV -------> too\n",
      "hot -------> ADJ -------> hot\n",
      "the -------> DET -------> the\n",
      "eye -------> NOUN -------> eye\n",
      "of -------> ADP -------> of\n",
      "heaven -------> PROPN -------> heaven\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:50]:\n",
    "    print(token.text,\"------->\",token.pos_, \"------->\",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2c80c-54a7-4ecf-a78d-74c50f6e6697",
   "metadata": {},
   "source": [
    "> Points to be noted:  \n",
    "> - Lemma for a particular word is determined by keeping in mind the **part-of-speech**. Thus, we can verify Lemmitization of a word also depends upon the parts-of-speech.\n",
    "> - Also, Lemmatization doesn't reduce words to their most basic synonym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41d362-f174-4f12-9a36-3e258175935f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc14c358-f01b-4d95-94fd-1d22175d5cfe",
   "metadata": {},
   "source": [
    "## 3. Stopwards in spaCy  \n",
    "There are many words in the English dictionary that are very common and are of no important use to us for finding some useful information from them. These can be words like- is, am ,are, a, an, the, etc. If we keep them as it is in the text, they will tend to increase the vocabulary size and making use of these large size vocab to train our model can be really time taking and also our model can become inefficient as these gie no useful information.  \n",
    "\n",
    "**spaCy holds an in-built set of 326 stopwords in the English by default, that are removed from our text document in the preproceesing pipeline. These stopwards are-**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e710b535-391f-4ddc-b0fe-f8f814439d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] length of built in stopwaords:  326\n",
      "[INFO] list of the stopwords \n",
      " {'all', 'already', 'further', 'say', 'what', 'former', 'will', 'sometime', 'that', 'a', 'two', 'really', 'top', 'almost', 'rather', 'amongst', 'themselves', 'anywhere', 'whereafter', 'very', 'give', 'wherever', 'well', 'if', 'none', '‘re', 'whether', 'among', 'nine', 'over', 'does', 'own', 'whatever', 'bottom', 'mine', 'between', 'at', 'why', 'which', 'across', 'whereby', 'became', 'make', 'every', 'must', 'your', 'there', 'though', 'throughout', 'am', 'from', 'n‘t', 'latterly', 'the', 'hence', \"n't\", 'until', 'whither', 'one', 'another', 'keep', \"'m\", 'empty', 'ca', 'seeming', 'than', 'through', 'itself', 'beside', 'into', 'ours', 'such', 'somewhere', 'amount', 'might', 'front', 'us', 'enough', 'not', 'since', 'fifteen', 'indeed', 'latter', 'meanwhile', 'her', 'therein', 'thus', 'seems', 'while', 'nobody', 'could', 'about', 'cannot', 'serious', 'becomes', \"'ve\", 'least', 'hundred', 'would', 'fifty', 'on', 'with', 'however', 'anyhow', 'third', 'still', '‘d', 'unless', 'nothing', 'whoever', 'less', 'no', 'an', 'quite', 'neither', 'these', 'he', 'where', 'you', 'ourselves', 'they', '’ve', 'being', 'elsewhere', 'whole', 'my', 'somehow', 'forty', '’ll', 'was', 'next', 'this', 'else', 'any', 'often', 'although', 'me', 'sixty', 'of', 'side', 'noone', 'either', 'much', 'someone', 'them', 'always', 'also', '’d', 'besides', 'we', 'nevertheless', 'so', 'onto', 'three', 'beforehand', 'hereafter', 'to', 'been', 'thereupon', 'are', 'their', 'everyone', '‘ve', '’re', 'down', 'as', 'did', 'anyone', 'five', 'some', 'seem', 'show', \"'re\", 'around', 'whom', 'off', 'wherein', 'in', 'thereafter', 'anything', 'now', 'along', 'toward', 'via', 'other', 'call', 'ten', 'once', 'whence', 'doing', 'hers', 'during', 'beyond', 'used', 'n’t', 'eleven', 'moreover', 'whereas', 'whereupon', 'just', 'has', 'never', 'himself', 'full', 'each', 'namely', 'hereupon', 'myself', 'against', 'anyway', 'because', 'name', '‘m', 'up', 'part', 'above', 'have', 'his', 'or', 'using', 'herein', 'seemed', 'upon', 'our', 'out', 'six', 'otherwise', 'thru', 'she', 'hereby', 'others', 'within', '‘ll', 'here', 'done', 'were', 'mostly', 'towards', 'four', 'go', 'its', 'put', 'those', 'therefore', 'due', 'may', 'per', 'only', 'sometimes', 'how', 'many', 'please', 'him', \"'ll\", 'become', 'yet', 'formerly', 'when', 'but', 'back', 'something', 'by', 'nowhere', '‘s', 'i', 'even', 'then', 'can', 'again', \"'d\", 'who', 'without', 'various', 'whose', '’s', 'yourselves', 'had', 'twelve', 'before', 'several', 'both', 'for', 'whenever', 'be', 'more', \"'s\", 'herself', 'is', 'see', 'eight', 'first', 'yourself', 'afterwards', 'ever', 'regarding', 'after', 'thence', 'take', 'should', 'alone', 'do', 'it', 'made', 'yours', 'everywhere', 'and', 'nor', 'thereby', 'move', 'under', 'same', 'except', 'behind', 'becoming', 'last', 'perhaps', 'everything', 'twenty', 'below', 'too', 'get', 'together', 'few', '’m', 're', 'most'}\n"
     ]
    }
   ],
   "source": [
    "print('[INFO] length of built in stopwaords: ', len(spacy_obj.Defaults.stop_words))\n",
    "print('[INFO] list of the stopwords \\n', spacy_obj.Defaults.stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e21b39db-1d04-4658-abf6-4db63a45a6c0",
   "metadata": {},
   "source": [
    "**Removal of stopwards from the text data**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "174ea14c-ea54-4c61-9851-f24d2555a459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[INFO] original token list: \n",
      " ['oh', 'brick', '.', 'i', 'get', 'so', 'lonely', '.', 'living', 'with', 'someone', 'you', 'love', 'can', 'be', 'lonelier', 'than', 'living', 'entirely', 'alone', 'when', 'the', 'one', 'you', 'love', 'does', 'n’t', 'love', 'you', '.', 'you', 'ca', 'n’t', 'even', 'stand', 'drinking', 'out', 'of', 'the', 'same', 'glass', 'can', 'you', '?', '…', 'no', '!', 'no', ',', 'i', 'would', 'n’t', '.', 'why', 'ca', 'n’t', 'you', 'lose', 'your', 'good', 'looks', 'brick', '?', 'most', 'drinking', 'men', 'lose', 'theirs', '.', 'why', 'ca', 'n’t', 'you', '.', 'i', 'think', 'you', '’ve', 'even', 'gotten', 'better', 'looking', 'since', 'you', 'were', 'n’t', 'on', 'the', 'bottle', '.', 'you', 'were', 'such', 'a', 'wonderful', 'love', '.', '…', 'you', 'were', 'so', 'exciting', 'to', 'be', 'in', 'love', 'with', '.', 'mostly', 'i', 'guess', 'because', 'you', 'were', '…', 'if', 'i', 'thought', 'you', '’d', 'never', 'never', 'made', 'love', 'to', 'me', 'again', ',', 'why', 'i', '’d', 'find', 'me', 'the', 'longest', 'sharpest', 'knife', 'i', 'could', 'and', 'i', '’d', 'stick', 'it', 'straight', 'into', 'my', 'heart', '.', 'i', '’d', 'do', 'that', '.', 'oh', 'brick', 'how', 'long', 'does', 'this', 'have', 'to', 'go', 'on', ',', 'this', 'punishment', '?', 'have', 'n’t', 'i', 'served', 'my', 'term', '?', 'ca', 'n’t', 'i', 'apply', 'for', 'a', 'pardon', '?', '…', 'is', 'it', 'any', 'wonder', '.', 'you', 'know', 'what', 'i', 'feel', 'like', '?', 'i', 'feel', 'all', 'the', 'time', 'like', 'a', 'cat', 'on', 'a', 'hot', 'tin', 'roof', '.']\n",
      "\n",
      "[INFO] token list after stopword removal: \n",
      " ['oh', 'brick', '.', 'lonely', '.', 'living', 'love', 'lonelier', 'living', 'entirely', 'love', 'love', '.', 'stand', 'drinking', 'glass', '?', '…', '!', ',', '.', 'lose', 'good', 'looks', 'brick', '?', 'drinking', 'men', 'lose', '.', '.', 'think', 'gotten', 'better', 'looking', 'bottle', '.', 'wonderful', 'love', '.', '…', 'exciting', 'love', '.', 'guess', '…', 'thought', 'love', ',', 'find', 'longest', 'sharpest', 'knife', 'stick', 'straight', 'heart', '.', '.', 'oh', 'brick', 'long', ',', 'punishment', '?', 'served', 'term', '?', 'apply', 'pardon', '?', '…', 'wonder', '.', 'know', 'feel', 'like', '?', 'feel', 'time', 'like', 'cat', 'hot', 'tin', 'roof', '.']\n",
      "\n",
      "[INFO] Actual data after stopword removal is: \n",
      " oh brick . lonely . living love lonelier living entirely love love . stand drinking glass ? … ! , . lose good looks brick ? drinking men lose . . think gotten better looking bottle . wonderful love . … exciting love . guess … thought love , find longest sharpest knife stick straight heart . . oh brick long , punishment ? served term ? apply pardon ? … wonder . know feel like ? feel time like cat hot tin roof .\n"
     ]
    }
   ],
   "source": [
    "textual_data=spacy_obj(\"Oh Brick. I get so lonely. Living with someone you love can be lonelier than living entirely alone when the one you love doesn’t love you. You can’t even stand drinking out of the same glass can you? … No! No, I wouldn’t. Why can’t you lose your good looks Brick? Most drinking men lose theirs. Why can’t you. I think you’ve even gotten better looking since you weren’t on the bottle. You were such a wonderful love. … You were so exciting to be in love with. Mostly I guess because you were … If I thought you’d never never made love to me again, why I’d find me the longest sharpest knife I could and I’d stick it straight into my heart. I’d do that. Oh Brick how long does this have to go on, this punishment? Haven’t I served my term? Can’t I apply for a pardon? … Is it any wonder. You know what I feel like? I feel all the time like a cat on a hot tin roof.\")\n",
    "\n",
    "all_stopwords=spacy_obj.Defaults.stop_words  \n",
    "\n",
    "# Defining a remoe stopwords function to remove all the stopwords from the text  \n",
    "def remove_stopwords(data):\n",
    "    # Creating an empty list that will store all individual tokens \n",
    "    all_tokens=[] \n",
    "    \n",
    "    for token in textual_data:\n",
    "        all_tokens.append(token.text.lower())\n",
    "\n",
    "    print(\"\\n[INFO] original token list: \\n\", all_tokens)\n",
    "    \n",
    "    \n",
    "    tokens_left = [word for word in all_tokens if not word in all_stopwords]\n",
    "    print(\"\\n[INFO] token list after stopword removal: \\n\", tokens_left)\n",
    "    \n",
    "    refined_textual_data=(' ').join(tokens_left)\n",
    "    \n",
    "    return refined_textual_data\n",
    "\n",
    "print('\\n[INFO] Actual data after stopword removal is: \\n', remove_stopwords(textual_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec1053-f621-4ebb-a3f6-fc9099aed34f",
   "metadata": {},
   "source": [
    "**Adding a custom stopword for removal from the text**  \n",
    "\n",
    "Suppose there are certain cases where our text data consists of some irrelevant words that are of no use to us and we want to remove those words from the text data and these words do not exist in the in-built stopword set. So, for such cases spaCy gives us the freedom to add our custom stopwords to the default set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0e0a4c75-7632-4112-8716-3341bb80f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] added extra stopword  327\n",
      "\n",
      "[INFO] original token list: \n",
      " ['oh', 'brick', '.', 'i', 'get', 'so', 'lonely', '.', 'living', 'with', 'someone', 'you', 'love', 'can', 'be', 'lonelier', 'than', 'living', 'entirely', 'alone', 'when', 'the', 'one', 'you', 'love', 'does', 'n’t', 'love', 'you', '.', 'you', 'ca', 'n’t', 'even', 'stand', 'drinking', 'out', 'of', 'the', 'same', 'glass', 'can', 'you', '?', '…', 'no', '!', 'no', ',', 'i', 'would', 'n’t', '.', 'why', 'ca', 'n’t', 'you', 'lose', 'your', 'good', 'looks', 'brick', '?', 'most', 'drinking', 'men', 'lose', 'theirs', '.', 'why', 'ca', 'n’t', 'you', '.', 'i', 'think', 'you', '’ve', 'even', 'gotten', 'better', 'looking', 'since', 'you', 'were', 'n’t', 'on', 'the', 'bottle', '.', 'you', 'were', 'such', 'a', 'wonderful', 'love', '.', '…', 'you', 'were', 'so', 'exciting', 'to', 'be', 'in', 'love', 'with', '.', 'mostly', 'i', 'guess', 'because', 'you', 'were', '…', 'if', 'i', 'thought', 'you', '’d', 'never', 'never', 'made', 'love', 'to', 'me', 'again', ',', 'why', 'i', '’d', 'find', 'me', 'the', 'longest', 'sharpest', 'knife', 'i', 'could', 'and', 'i', '’d', 'stick', 'it', 'straight', 'into', 'my', 'heart', '.', 'i', '’d', 'do', 'that', '.', 'oh', 'brick', 'how', 'long', 'does', 'this', 'have', 'to', 'go', 'on', ',', 'this', 'punishment', '?', 'have', 'n’t', 'i', 'served', 'my', 'term', '?', 'ca', 'n’t', 'i', 'apply', 'for', 'a', 'pardon', '?', '…', 'is', 'it', 'any', 'wonder', '.', 'you', 'know', 'what', 'i', 'feel', 'like', '?', 'i', 'feel', 'all', 'the', 'time', 'like', 'a', 'cat', 'on', 'a', 'hot', 'tin', 'roof', '.']\n",
      "\n",
      "[INFO] token list after stopword removal: \n",
      " ['oh', 'brick', '.', 'lonely', '.', 'living', 'love', 'lonelier', 'living', 'entirely', 'love', 'love', '.', 'stand', 'drinking', 'glass', '?', '…', '!', ',', '.', 'lose', 'good', 'looks', 'brick', '?', 'drinking', 'men', 'lose', '.', '.', 'think', 'gotten', 'better', 'looking', 'bottle', '.', 'wonderful', 'love', '.', '…', 'exciting', 'love', '.', 'guess', '…', 'thought', 'love', ',', 'find', 'longest', 'sharpest', 'knife', 'stick', 'straight', 'heart', '.', '.', 'oh', 'brick', 'long', ',', 'punishment', '?', 'served', 'term', '?', 'apply', 'pardon', '?', '…', 'wonder', '.', 'know', 'feel', 'like', '?', 'feel', 'time', 'like', 'cat', 'hot', 'tin', 'roof', '.']\n",
      "[INFO] \n",
      "Actual data after stop word removal is : \n",
      " oh brick . lonely . living love lonelier living entirely love love . stand drinking glass ? … ! , . lose good looks brick ? drinking men lose . . think gotten better looking bottle . wonderful love . … exciting love . guess … thought love , find longest sharpest knife stick straight heart . . oh brick long , punishment ? served term ? apply pardon ? … wonder . know feel like ? feel time like cat hot tin roof .\n"
     ]
    }
   ],
   "source": [
    "spacy_obj.Defaults.stop_words.add('theirs')\n",
    "spacy_obj.vocab['theirs'].is_stop = True\n",
    "\n",
    "# Verifying that our custom stopwords are added into the default list\n",
    "print('[INFO] added extra stopword ', len(spacy_obj.Defaults.stop_words))\n",
    "\n",
    "# Calling the remove_stopwords function again on the textual_data\n",
    "print(\"[INFO] \\nActual data after stop word removal is : \\n\",remove_stopwords(textual_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6aab53-299e-40cd-873f-d12a1c2a1a3c",
   "metadata": {},
   "source": [
    "**Removing a stopword from the stopword set**  \n",
    "\n",
    "If we want to remove some stopwords from the by default stopword set, we can also do it. It can be done as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dc640bb8-a850-4d7d-a79e-a5e387e245e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] removed the stopword we added, length should be original  326\n"
     ]
    }
   ],
   "source": [
    "spacy_obj.Defaults.stop_words.remove('theirs')\n",
    "spacy_obj.vocab['theirs'].is_stop = False\n",
    "# Verifying that our custom stopwords we added are removed from the default list\n",
    "print('[INFO] removed the stopword we added, length should be original ',len(spacy_obj.Defaults.stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf29d42-6842-4d80-8092-dbf7eaff6fef",
   "metadata": {},
   "source": [
    "---  \n",
    "\n",
    "## 4. Parts of Speech tagging (POS tagging)  \n",
    "The next step in the preprocessing pipeline after tokenisation is to assign appropriate parts-of-speech to each token. This step can be useful in many NLP tasks for information extraction, feature engineering, language understanding, etc. As discussed earlier spaCy library is already trained with statistical models that allows it to achieve the objective of POS tagging efficiently. The statistical model contains binary data and is already trained over a lot of examples that enables it to make generalized predictions.  \n",
    "  \n",
    "### Different types of POS tags available in spaCy  \n",
    "There are two types of POS tags available in spaCy that are-  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a9e7c3-0f76-4df1-bb03-34392fd72d6e",
   "metadata": {},
   "source": [
    "**(a) coarse POS tags**    \n",
    "These are ordinary POS tags that we know. Each token is assigned a its own coarse POS tag.  \n",
    "`{ ADJ, ADP, ADV, AUX, CONJ, CCONJ, DET, INTJ, NOUN }  `    \n",
    "  \n",
    "![coarse POS tags](../../pos_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cbe2936-2982-4c51-873e-a6480898786c",
   "metadata": {},
   "source": [
    "**(b) fine-grained tags**  \n",
    "In this each token is assigned a more detailed POS tag based on the **morphology**.  \n",
    "The full list can be found on the documentation page.  \n",
    "  \n",
    "![fine-grained tags](../../pos_2.png)  \n",
    "  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513d16b5-38b9-43fa-9d6a-82d79e7478e3",
   "metadata": {},
   "source": [
    "Now that we have gained some knowledge about various POS tags available in spaCy. Let's get into skin of it by working on some hands-on examples. For the text that we read earlier, we will be generating the POS tags for each token in that text data. We can make use of the `pos_` tag and `tag_` tag along with each token to view its respective coarse and fine-grained POS tag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e4203bb4-5a98-4a47-a2f4-38c8909ac4ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Token 'Look'  its coarse tag is:   VERB  and it's fine-grained POS tag is: VB\n",
      "[INFO] Token 'in'  its coarse tag is:   ADP  and it's fine-grained POS tag is: IN\n",
      "[INFO] Token 'thy'  its coarse tag is:   PRON  and it's fine-grained POS tag is: PRP$\n",
      "[INFO] Token 'glass'  its coarse tag is:   NOUN  and it's fine-grained POS tag is: NN\n",
      "[INFO] Token ','  its coarse tag is:   PUNCT  and it's fine-grained POS tag is: ,\n",
      "[INFO] Token 'and'  its coarse tag is:   CCONJ  and it's fine-grained POS tag is: CC\n",
      "[INFO] Token 'tell'  its coarse tag is:   VERB  and it's fine-grained POS tag is: VB\n",
      "[INFO] Token 'the'  its coarse tag is:   DET  and it's fine-grained POS tag is: DT\n",
      "[INFO] Token 'face'  its coarse tag is:   NOUN  and it's fine-grained POS tag is: NN\n",
      "[INFO] Token 'thou'  its coarse tag is:   DET  and it's fine-grained POS tag is: DT\n",
      "[INFO] Token 'viewest'  its coarse tag is:   NOUN  and it's fine-grained POS tag is: NN\n",
      "[INFO] Token 'Now'  its coarse tag is:   ADV  and it's fine-grained POS tag is: RB\n",
      "[INFO] Token 'is'  its coarse tag is:   AUX  and it's fine-grained POS tag is: VBZ\n",
      "[INFO] Token 'the'  its coarse tag is:   DET  and it's fine-grained POS tag is: DT\n",
      "[INFO] Token 'time'  its coarse tag is:   NOUN  and it's fine-grained POS tag is: NN\n",
      "[INFO] Token 'that'  its coarse tag is:   DET  and it's fine-grained POS tag is: WDT\n",
      "[INFO] Token 'face'  its coarse tag is:   NOUN  and it's fine-grained POS tag is: NN\n",
      "[INFO] Token 'should'  its coarse tag is:   AUX  and it's fine-grained POS tag is: MD\n",
      "[INFO] Token 'form'  its coarse tag is:   VERB  and it's fine-grained POS tag is: VB\n",
      "[INFO] Token 'another'  its coarse tag is:   DET  and it's fine-grained POS tag is: DT\n",
      "[INFO] Token ';'  its coarse tag is:   PUNCT  and it's fine-grained POS tag is: :\n",
      "[INFO] Token '.'  its coarse tag is:   PUNCT  and it's fine-grained POS tag is: .\n"
     ]
    }
   ],
   "source": [
    "text_1 = \"Look in thy glass, and tell the face thou viewest Now is the time that face should form another;.\"\n",
    "\n",
    "doc_1 = spacy_obj(text_1)\n",
    "\n",
    "for token in doc_1:\n",
    "    print( \"[INFO] Token '\"+token.text + \"'  its coarse tag is:   \"+ token.pos_ + \"  and it's fine-grained POS tag is: \" + token.tag_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23955202-e452-4396-93f4-fcbf98ad430d",
   "metadata": {},
   "source": [
    "We can use the `spacy.explain()` function to see the full information about the tag used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f110e158-dab2-4e91-85a5-45aa313249ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adposition\n",
      "determiner\n",
      "noun, proper singular\n"
     ]
    }
   ],
   "source": [
    "print(spacy.explain('ADP'))\n",
    "print(spacy.explain('DET'))\n",
    "print(spacy.explain('NNP'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8d68b54-f700-4125-814d-fae88ebf1ce3",
   "metadata": {},
   "source": [
    "It should be noted down that spaCy encodes all the strings token to a unique hash value in order to reduce memory usage and improve its efficiency. So, we can use the `pos` tag and `tag` tag to view the hash values. By using these tags, we can get the hash values of he corresponsding token. Also, these are the short hand notations of the two tags we read earlier that are `pos_` and `tag_`. We add an `underscore (_)` sign with these tag names so that we can get the hash values in readable string format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "932e05ba-09ce-40dc-a05d-fbd026305795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Look has coarse POS hash value: 100\n",
      "Look has fine-grained POS hash value: 14200088355797579614\n"
     ]
    }
   ],
   "source": [
    "# Printing the hash value of both the coarse POS tag and fine-grained POS tag for the very first token in doc_1.\n",
    "\n",
    "print(doc_1[0].text + ' has coarse POS hash value: ' + str(doc_1[0].pos))\n",
    "print(doc_1[0].text + ' has fine-grained POS hash value: '+ str(doc_1[0].tag))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c649e0-311e-4123-9409-eb9b8370fbb5",
   "metadata": {},
   "source": [
    "It is possible that the same words in two different sentences can give different meanings. To determine the real essence of each sentence spaCy library uses something known as **morphology**.\n",
    "\n",
    "Let's have two sentences and analyze their results after doing POS tagging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c02e613b-8dc0-4bf2-85c7-2f43e3146f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] sentence 1 POS tags:  read VERB VB verb, base form\n",
      "[INFO] sentence 2 POS tags:  read VERB VBD verb, past tense\n"
     ]
    }
   ],
   "source": [
    "sent_1 = spacy_obj('Jesse loves to read books on Computer Vision.')\n",
    "sent_2 = spacy_obj('Jesse read a book yesterday.')\n",
    "\n",
    "word_1 = sent_1[3] # Assigning word_1 = read\n",
    "word_2 = sent_2[1] # Assigning word_2 = read\n",
    "\n",
    "#printing POS tags for read token in the first sentence\n",
    "print('[INFO] sentence 1 POS tags: ', word_1.text,word_1.pos_, word_1.tag_, spacy.explain(word_1.tag_))\n",
    "\n",
    "#printing POS tags for read token in the second sentence\n",
    "print('[INFO] sentence 2 POS tags: ', word_2.text,word_2.pos_, word_2.tag_, spacy.explain(word_2.tag_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4900393-8c08-41cb-ad15-a5c92a097ffc",
   "metadata": {},
   "source": [
    "If we analyze the result of the tagging, we can see spaCy was able to identify correct form of the common word in both the sentence correctly. It is possible because it uses morphology to determine the correct essence of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d9c38-f7d2-4cfa-ac11-0117aee75d28",
   "metadata": {},
   "source": [
    "---  \n",
    "## 5. Dependency Parsing  \n",
    "Dependency Parsing is a method which we can extract dependencies of a sentence that helps to represent the grammatical structure of the sentence. The dependency usually exists between the root word of the sentence and all other existing words. Usually, the 'Verb' inside the sentence is treated as a root word.  \n",
    "  \n",
    "The dependency amongst the words in a sentence can be represented using a directed graph. Various components that a directed graph represents are listed below:  \n",
    "- Each token(word) is represented using a node.  \n",
    "- The existing grammatical relation between two tokens is represented using edges.  \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e8e7f502-80db-48e2-8d06-fc9a6c05c2a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"455ab9d0f179415faea7a313065c2cfa-0\" class=\"displacy\" width=\"650\" height=\"237.0\" direction=\"ltr\" style=\"max-width: none; height: 237.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">Jesse</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PROPN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"150\">loves</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"150\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"250\">to</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"250\">PART</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"350\">read</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"350\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"450\">comic</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"450\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"147.0\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"550\">books.</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"550\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-455ab9d0f179415faea7a313065c2cfa-0-0\" stroke-width=\"2px\" d=\"M70,102.0 C70,52.0 145.0,52.0 145.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-455ab9d0f179415faea7a313065c2cfa-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,104.0 L62,92.0 78,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-455ab9d0f179415faea7a313065c2cfa-0-1\" stroke-width=\"2px\" d=\"M270,102.0 C270,52.0 345.0,52.0 345.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-455ab9d0f179415faea7a313065c2cfa-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M270,104.0 L262,92.0 278,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-455ab9d0f179415faea7a313065c2cfa-0-2\" stroke-width=\"2px\" d=\"M170,102.0 C170,2.0 350.0,2.0 350.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-455ab9d0f179415faea7a313065c2cfa-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M350.0,104.0 L358.0,92.0 342.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-455ab9d0f179415faea7a313065c2cfa-0-3\" stroke-width=\"2px\" d=\"M470,102.0 C470,52.0 545.0,52.0 545.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-455ab9d0f179415faea7a313065c2cfa-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">amod</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M470,104.0 L462,92.0 478,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-455ab9d0f179415faea7a313065c2cfa-0-4\" stroke-width=\"2px\" d=\"M370,102.0 C370,2.0 550.0,2.0 550.0,102.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-455ab9d0f179415faea7a313065c2cfa-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M550.0,104.0 L558.0,92.0 542.0,92.0\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Visualizing the Parts Of Speech using the displaCy\n",
    "\n",
    "from spacy import displacy\n",
    "\n",
    "data = spacy_obj('Jesse loves to read comic books.')\n",
    "displacy.render(data, style='dep', jupyter=True, options={'distance': 100})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d0ac9d-7e20-4991-8b89-a4abb43c4b07",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc46249-01ae-411f-86e0-a5e92b21c482",
   "metadata": {},
   "source": [
    "## References:  \n",
    "https://www.kaggle.com/saurabh48782/nlp-using-spacy-library-part-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c3356a-7ee1-416e-ada5-e27f76b84995",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
