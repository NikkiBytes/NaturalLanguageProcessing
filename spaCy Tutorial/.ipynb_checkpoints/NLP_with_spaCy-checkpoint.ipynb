{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6984153-7297-4518-a6e5-98af028cf63d",
   "metadata": {},
   "source": [
    "# NLP with spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "723a1e67-5da3-4f65-be30-f51ffa6ba370",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1feed3-83e9-41ff-9367-9e44f6136c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy_obj=spacy.load('en_core_web_sm') # create a spaCy obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b117166-d36d-4328-9be8-4a25c4efc33b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x260dddfa6d0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x260e0c1e860>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x260df097580>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x260df097520>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x260e0cb76c0>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x260e0cb9cc0>)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_obj.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d8a9c7f-1fe5-4959-ad1d-d65809d1756b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spacy_obj.pipe_names # view the pipeline component names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed04d26-3dcb-4b45-abbf-3bcd27dd73fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to disable pipeline components we use the disable_pipes()\n",
    "#spacy_obj.disable('attribute_ruler', 'tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "49571cb0-d4b4-4270-8fc3-95a48d16dab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read a text into the spaCy object  \n",
    "text='''Shall I compare thee to a summer’s day?\n",
    "Thou art more lovely and more temperate:\n",
    "Rough winds do shake the darling buds of May,\n",
    "And summer’s lease hath all too short a date:\n",
    "Sometime too hot the eye of heaven shines,\n",
    "And often is his gold complexion dimm’d;\n",
    "And every fair from fair sometime declines,\n",
    "By chance or nature’s changing course untrimm’d;\n",
    "But thy eternal summer shall not fade\n",
    "Nor lose possession of that fair thou owest;\n",
    "Nor shall Death brag thou wander’st in his shade,\n",
    "When in eternal lines to time thou growest:\n",
    "So long as men can breathe or eyes can see,\n",
    "So long lives this and this gives life to thee.\n",
    "'''\n",
    "\n",
    "doc=spacy_obj(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba489520-ebf4-4b20-9c53-cc8af5af7d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a summer’s day?\n",
      "Thou art more lovely and more temperate:\n",
      "Rough winds do shake the darling buds of May,\n",
      "And summer’s lease hath all too short a date:\n",
      "Sometime too hot the eye of heaven shines,\n",
      "And often is his gold complexion dimm’d;\n",
      "And every fair from fair sometime declines,\n",
      "By chance or nature’s changing course untrimm’d;\n",
      "But thy eternal summer shall not fade\n",
      "Nor lose possession of that fair thou owest;\n",
      "Nor shall Death brag thou wander’st in his shade,\n",
      "When in eternal lines to time thou growest:\n",
      "So long as men can breathe or eyes can see,\n",
      "So long lives this and this gives life to thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46145387-ddfc-43dd-9be9-cf2d0dab136a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f30db96b-2ccf-4563-9174-aefbe19287b4",
   "metadata": {},
   "source": [
    "## 1. Tokenization  \n",
    "Split strings into tokens. Tokens can be a word or a character.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff108cf-2770-4167-ada7-fb90863c008d",
   "metadata": {},
   "source": [
    "**(A) Tokenization into sentences**  \n",
    "Use the `'sents'` attribute to tokenize the text doc into sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f836613-90cf-45a8-8869-f884b73125e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a summer’s day?\n",
      "\n",
      "\n",
      "Thou art more lovely and more temperate:\n",
      "\n",
      "Rough winds do shake the darling buds of May,\n",
      "\n",
      "And summer’s lease hath all too short a date:\n",
      "\n",
      "Sometime too hot the eye of heaven shines,\n",
      "And often is his gold complexion dimm’d;\n",
      "\n",
      "And every fair from fair sometime declines,\n",
      "\n",
      "By chance or nature’s changing course untrimm’d;\n",
      "\n",
      "But thy eternal summer shall not fade\n",
      "\n",
      "Nor lose possession of that fair thou owest;\n",
      "\n",
      "Nor shall Death brag thou wander’st in his shade,\n",
      "\n",
      "When in eternal lines to time thou growest:\n",
      "So long as men can breathe or eyes can see,\n",
      "\n",
      "So long lives this and this gives life to thee.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentences in doc.sents:\n",
    "    print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e4566c-2c85-4a43-99e7-3d5f30e2d0d2",
   "metadata": {},
   "source": [
    "**(B) Tokenization into individual words**  \n",
    "This will divide the entire sentence into word by word, including the punctuation marks and escape sentences.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5493d93-4694-40d6-82dd-e981a6dc593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall\n",
      "I\n",
      "compare\n",
      "thee\n",
      "to\n",
      "a\n",
      "summer\n",
      "’s\n",
      "day\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:10]:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678b6674-ef5d-4cfe-b3af-448b85fcdf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall\n",
      "thee\n"
     ]
    }
   ],
   "source": [
    "# print individual tokens by using the slicing notation\n",
    "print(doc[0])\n",
    "print(doc[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8038d-5253-4770-91b3-71f3e2bbf413",
   "metadata": {},
   "source": [
    "spaCy follows a specific rule for tokenization of text  \n",
    "1. Initially it starts preprocessing the raw text into tokens from left to right based on whitespaces.  \n",
    "2. Then it performs spliting tokens into sub-tokens by performing two different checks that are:  \n",
    "  - (a) Exception rule check: punctuation marks in between tokens are looked over and are left untokenised.  \n",
    "  - (b) Prefix-Suffix and Infix check: punctuation marks like commas, hyphens, quotation marks, periods, etc. are identified and made as a separate token.  \n",
    "  \n",
    "    \n",
    "This rule checking is applied iteratively on the tokens from left to right. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0a0f7383-93b0-42e7-8791-7072eeb54223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "love\n",
      "cats\n",
      "and\n",
      "coffee\n",
      ",\n",
      "\"\n",
      "I\n",
      "'ll\n",
      "get\n",
      "a\n",
      "big\n",
      "house\n",
      "for\n",
      "lots\n",
      "of\n",
      "cats\n",
      "in\n",
      "the\n",
      "U.S.A\n",
      "\"\n"
     ]
    }
   ],
   "source": [
    "test_text=spacy_obj(\"I love cats and coffee, \\\"I'll get a big house for lots of cats in the U.S.A \\\"\")\n",
    "\n",
    "for token in test_text:\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0545095-160e-4c9f-9d39-bbdd4c2f02b2",
   "metadata": {},
   "source": [
    "- spaCy identifies each quotation mark, commas, question mark and other punctuation marks that are present in form of prefix, suffix, infix and separates them into an indvidual token. (In above example, I'll be was separated into 'I & 'll' two different tokens.)    \n",
    "- Punctuation marks that exists as part of a known abbereviation will not be separated. (In above example, U.S.A is kept as U.S.A.)  \n",
    "-Also, punctuation marks used as infixes will be exempted from tokenisation in cases of email address, website or some numerical figures."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8133342-3b9d-41a2-9ff2-027d702e82d8",
   "metadata": {},
   "source": [
    "**(C) Spanning/Slicing of words in the text**  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d12575ae-6c61-4bb2-99b0-d7ce3d18ac21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall I compare thee to a summer\n"
     ]
    }
   ],
   "source": [
    "print(doc[0:7])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a7ad76-c1c4-4833-b01e-8a08ec74df8c",
   "metadata": {},
   "source": [
    "To check whether a particular word is the starting of a sentence we can use the 'is_sent_start' attribute with the doc object. It returns a Boolean value, True in case the word is the starting of any sentence otherwise it returns False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a7100847-4f20-4e45-8234-3f313494cad1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(doc[0].is_sent_start)\n",
    "print(doc[1].is_sent_start)\n",
    "print(doc[10].is_sent_start)\n",
    "print(doc[18].is_sent_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf52fab8-67ee-48b1-a9e5-98c33f683f16",
   "metadata": {},
   "source": [
    "**(D) Assignment of tokens is not allowed in spaCy.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "07a000e2-f385-4dae-9a8d-e0c3e2fc66a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc[0] = doc[1] # will throw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5764fb7e-3046-4149-b3f0-0c3dd076923f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60199e41-350b-4cff-bb2e-6b1177cfc453",
   "metadata": {},
   "source": [
    "## 2. Lemmatization  \n",
    "Lemmatization is the process of grouping inflected words from a common root word. This groups them into a single term for analysis. It considers the language's full vocabulary so it can apply morphological analysis on words.  \n",
    "  \n",
    "**The point to be noted is that spaCy library does not have stemming feature, because it prefers lemmitization as it is considerd to be more informative than stemming.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "270a5140-255e-4e04-aafa-98fe7737f18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shall -------> AUX -------> shall\n",
      "I -------> PRON -------> I\n",
      "compare -------> VERB -------> compare\n",
      "thee -------> PRON -------> thee\n",
      "to -------> ADP -------> to\n",
      "a -------> DET -------> a\n",
      "summer -------> NOUN -------> summer\n",
      "’s -------> PART -------> ’s\n",
      "day -------> NOUN -------> day\n",
      "? -------> PUNCT -------> ?\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "Thou -------> DET -------> thou\n",
      "art -------> NOUN -------> art\n",
      "more -------> ADV -------> more\n",
      "lovely -------> ADJ -------> lovely\n",
      "and -------> CCONJ -------> and\n",
      "more -------> ADV -------> more\n",
      "temperate -------> NOUN -------> temperate\n",
      ": -------> PUNCT -------> :\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "Rough -------> ADJ -------> rough\n",
      "winds -------> NOUN -------> wind\n",
      "do -------> AUX -------> do\n",
      "shake -------> VERB -------> shake\n",
      "the -------> DET -------> the\n",
      "darling -------> NOUN -------> darling\n",
      "buds -------> NOUN -------> bud\n",
      "of -------> ADP -------> of\n",
      "May -------> PROPN -------> May\n",
      ", -------> PUNCT -------> ,\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "And -------> CCONJ -------> and\n",
      "summer -------> NOUN -------> summer\n",
      "’s -------> PART -------> ’s\n",
      "lease -------> NOUN -------> lease\n",
      "hath -------> NOUN -------> hath\n",
      "all -------> ADV -------> all\n",
      "too -------> ADV -------> too\n",
      "short -------> ADJ -------> short\n",
      "a -------> DET -------> a\n",
      "date -------> NOUN -------> date\n",
      ": -------> PUNCT -------> :\n",
      "\n",
      " -------> SPACE -------> \n",
      "\n",
      "Sometime -------> ADV -------> sometime\n",
      "too -------> ADV -------> too\n",
      "hot -------> ADJ -------> hot\n",
      "the -------> DET -------> the\n",
      "eye -------> NOUN -------> eye\n",
      "of -------> ADP -------> of\n",
      "heaven -------> PROPN -------> heaven\n"
     ]
    }
   ],
   "source": [
    "for token in doc[:50]:\n",
    "    print(token.text,\"------->\",token.pos_, \"------->\",token.lemma_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c2c80c-54a7-4ecf-a78d-74c50f6e6697",
   "metadata": {},
   "source": [
    "> Points to be noted:  \n",
    "> - Lemma for a particular word is determined by keeping in mind the **part-of-speech**. Thus, we can verify Lemmitization of a word also depends upon the parts-of-speech.\n",
    "> - Also, Lemmatization doesn't reduce words to their most basic synonym."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c41d362-f174-4f12-9a36-3e258175935f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a73459c0-b330-4da5-831c-6cd3452a56c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
