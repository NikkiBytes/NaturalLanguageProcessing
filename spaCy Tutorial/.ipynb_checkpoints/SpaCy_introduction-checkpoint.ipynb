{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26737f91",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a5924",
   "metadata": {},
   "source": [
    "spaCy package: leading NLP library. \n",
    "- relies on models that are language-specific and come in different sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "79596a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ba93378",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm') # load English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42794685",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tea is healthy and I love cats and I don't like people messing with my kitties! \") # process text like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea80dd4",
   "metadata": {},
   "source": [
    "## Tokenizing  \n",
    "A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractins like \"don't\" into two tokens, \"do\" and \"n't\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4e46acc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "I\n",
      "love\n",
      "cats\n",
      "and\n",
      "I\n",
      "do\n",
      "n't\n",
      "like\n",
      "people\n",
      "messing\n",
      "with\n",
      "my\n",
      "kitties\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25a32919",
   "metadata": {},
   "source": [
    "Iterating through a document gives you token objects, and each of these tokens come with additional information.   \n",
    "`token.lemma_` and `token.is_stop` are important in many cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a4f80",
   "metadata": {},
   "source": [
    " ## Text Preprocessing  \n",
    "We need to preprocess to improve modeling of words.  \n",
    "The first is **lemmatizing**. The **lemma** of a word is its base form. A good example is *walk*, the lemma of the word *walking*. When you lemmatize the word walking, it would convert to walk.  \n",
    " \n",
    "Another common technique is to remove *stopwords*. These are the words that occur frequently in the language and don't contin much information. In English these include, \"the\", \"is\", \"and\", \"but\", \"not\".  \n",
    "  \n",
    "Using spaCy, `token.lemma_` returns the lemma, while `token.is_stop` returns a boolean `True` if the token is a stopword.  \n",
    "  \n",
    "    \n",
    "Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarily helps by combining multiple forms of the same word into a base form.  \n",
    "  \n",
    "However, lemmatizing and dropping stopwords might result in lower performance. Treat this preprocessing as part of your hyperparameter optimization process.  \n",
    "  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6b3b63f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "I\t\tI\t\tTrue\n",
      "love\t\tlove\t\tFalse\n",
      "cats\t\tcat\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "I\t\tI\t\tTrue\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tn't\t\tTrue\n",
      "like\t\tlike\t\tFalse\n",
      "people\t\tpeople\t\tFalse\n",
      "messing\t\tmess\t\tFalse\n",
      "with\t\twith\t\tTrue\n",
      "my\t\tmy\t\tTrue\n",
      "kitties\t\tkitty\t\tFalse\n",
      "!\t\t!\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cec4fb",
   "metadata": {},
   "source": [
    "## Pattern Matching  \n",
    "Another common NLP task is matching tokens and phrases with chunks of text or whole documents.   \n",
    "\n",
    "In spaCy, to match individual tokens, you create a `Matcher`. When matching a list of terms, it's easier and more efficient to use `PhraseMatcher`. For example, if you want to find where different smartphone models show up in some text, you can create patterns for the model names of interest.    \n",
    "To start, create the `PhraseMatcher`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf647c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.matcher import PhraseMatcher\n",
    "matcher = PhraseMatcher(nlp.vocab, attr='LOWER')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2763143",
   "metadata": {},
   "source": [
    "The matcher is created using the vocabulary of your model. Here we're using the small English model you loaded earlier. Setting attr='LOWER' will match the phrases on lowercased text. This provides case insensitve matching.  \n",
    "  \n",
    "Next, create a list of terms to match in the text. The phrase matcher needs the patterns as document objects. The easiest way to get these is with a list comprehension using the nlp model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "07bdf79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "terms = ['Galaxy Note', 'iPhone 11', 'iPhone XS', 'Google Pixel']\n",
    "patterns = [nlp(text) for text in terms]\n",
    "matcher.add('TerminologyList', patterns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306d8e29",
   "metadata": {},
   "source": [
    "Then you create a document from the text to search and use the phrase matcher to find where the terms occur in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b27dcab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3766102292120407359, 17, 19), (3766102292120407359, 22, 24), (3766102292120407359, 30, 32), (3766102292120407359, 33, 35)]\n"
     ]
    }
   ],
   "source": [
    "# Borrowed from https://daringfireball.net/linked/2019/09/21/patel-11-pro\n",
    "text_doc = nlp(\"Glowing review overall, and some really interesting side-by-side \"\n",
    "               \"photography tests pitting the iPhone 11 Pro against the \"\n",
    "               \"Galaxy Note 10 Plus and last yearâ€™s iPhone XS and Google Pixel 3.\")\n",
    "\n",
    "matches=matcher(text_doc)\n",
    "print(matches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfeb638b",
   "metadata": {},
   "source": [
    "The matches are a tuple: (match id, start postiion, end position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2b2aba4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TerminologyList iPhone 11\n"
     ]
    }
   ],
   "source": [
    "match_id, start, end=matches[0]\n",
    "print(nlp.vocab.strings[match_id], text_doc[start:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5408c8a",
   "metadata": {},
   "source": [
    "---\n",
    "## Reference\n",
    "https://www.kaggle.com/matleonard/intro-to-nlp  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
