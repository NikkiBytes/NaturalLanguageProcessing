{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74a53050",
   "metadata": {},
   "source": [
    "# Natural Language Processing (NLP) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41104f1f",
   "metadata": {},
   "source": [
    "spacy package: leading NLP library. \n",
    "- relies on models that are language-specific and come in different sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2297907b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "435a9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm') # load English language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "096a5c60",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(\"Tea is healthy and I love cats and I don't like people messing with my kitties! \") # process text like this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44598f3b",
   "metadata": {},
   "source": [
    "## Tokenizing  \n",
    "A token is a unit of text in the document, such as individual words and punctuation. SpaCy splits contractins like \"don't\" into two tokens, \"do\" and \"n't\". \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18ec204f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tea\n",
      "is\n",
      "healthy\n",
      "and\n",
      "I\n",
      "love\n",
      "cats\n",
      "and\n",
      "I\n",
      "do\n",
      "n't\n",
      "like\n",
      "people\n",
      "messing\n",
      "with\n",
      "my\n",
      "kitties\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "for token in doc:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8079a648",
   "metadata": {},
   "source": [
    "Iterating through a document gives you token objects, and each of these tokens come with additional information.   \n",
    "`token.lemma_` and `token.is_stop` are important in many cases.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8e3083",
   "metadata": {},
   "source": [
    " ## Text Preprocessing  \n",
    "We need to preprocess to improve modeling of words.  \n",
    "The first is **lemmatizing**. The **lemma** of a word is its base form. A good example is *walk*, the lemma of the word *walking*. When you lemmatize the word walking, it would convert to walk.  \n",
    " \n",
    "Another common technique is to remove *stopwords*. These are the words that occur frequently in the language and don't contin much information. In English these include, \"the\", \"is\", \"and\", \"but\", \"not\".  \n",
    "  \n",
    "Using spaCy, `token.lemma_` returns the lemma, while `token.is_stop` returns a boolean `True` if the token is a stopword.  \n",
    "  \n",
    "    \n",
    "Removing stop words might help the predictive model focus on relevant words. Lemmatizing similarily helps by combining multiple forms of the same word into a base form.  \n",
    "  \n",
    "However, lemmatizing and dropping stopwords might result in lower performance. Treat this preprocessing as part of your hyperparameter optimization process.  \n",
    "  \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6be862c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token \t\tLemma \t\tStopword\n",
      "----------------------------------------\n",
      "Tea\t\ttea\t\tFalse\n",
      "is\t\tbe\t\tTrue\n",
      "healthy\t\thealthy\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "I\t\tI\t\tTrue\n",
      "love\t\tlove\t\tFalse\n",
      "cats\t\tcat\t\tFalse\n",
      "and\t\tand\t\tTrue\n",
      "I\t\tI\t\tTrue\n",
      "do\t\tdo\t\tTrue\n",
      "n't\t\tn't\t\tTrue\n",
      "like\t\tlike\t\tFalse\n",
      "people\t\tpeople\t\tFalse\n",
      "messing\t\tmess\t\tFalse\n",
      "with\t\twith\t\tTrue\n",
      "my\t\tmy\t\tTrue\n",
      "kitties\t\tkitty\t\tFalse\n",
      "!\t\t!\t\tFalse\n"
     ]
    }
   ],
   "source": [
    "print(f\"Token \\t\\tLemma \\t\\tStopword\".format('Token', 'Lemma', 'Stopword'))\n",
    "print(\"-\"*40)\n",
    "for token in doc:\n",
    "    print(f\"{str(token)}\\t\\t{token.lemma_}\\t\\t{token.is_stop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3455d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4edde81",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ac146c4",
   "metadata": {},
   "source": [
    "# Reference\n",
    "https://www.kaggle.com/matleonard/intro-to-nlp  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
